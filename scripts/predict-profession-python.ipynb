{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import copy\n",
    "import dill\n",
    "\n",
    "\n",
    "# In[603]:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arguments: 3 arguments.\n",
      "Argument List: ['/home/andrei/anaconda2/envs/vocationcompass/lib/python3.7/site-packages/ipykernel_launcher.py', '-f', '/run/user/1000/jupyter/kernel-17731f65-6dc1-446e-bdac-325804645e5d.json']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/andrei/anaconda2/envs/vocationcompass/lib/python3.7/site-packages/ipykernel_launcher.py'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## in script mode, read filenames from arguments\n",
    "import sys\n",
    "\n",
    "print('Number of arguments:', len(sys.argv), 'arguments.')\n",
    "print('Argument List:', str(sys.argv))\n",
    "INPUT_FILE = str(sys.argv[1])\n",
    "OUTPUT_FILE = str(sys.argv[2])\n",
    "\n",
    "## comment the next two lines for Notebook mode\n",
    "# INPUT_FILE = \"../uniq_data.csv\"\n",
    "# OUTPUT_FILE = \"rand_optimised_all_R.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data in from the csv...\n",
      "Done reading data from csv\n",
      "Number of professions to run models on: 10\n",
      "Running with 10 folds\n",
      "Professions we are using now: \n",
      "Superintendent\n",
      "Manufacturer\n",
      "Campaigner\n",
      "School Principal\n",
      "Athletics Director\n",
      "Agent\n",
      "Data Scientist\n",
      "Teacher\n",
      "Software Engineer\n",
      "Executive Chef\n",
      "['Superintendent', 'Manufacturer', 'Campaigner', 'School Principal', 'Athletics Director', 'Agent', 'Data Scientist', 'Teacher', 'Software Engineer', 'Executive Chef']\n",
      "Total number of samples: 9550\n",
      "Splitting data into training and testing sets\n",
      "8590 960\n",
      "8590 960\n",
      "8590 960\n",
      "8590 960\n",
      "8590 960\n",
      "8600 950\n",
      "8600 950\n",
      "8600 950\n",
      "8600 950\n",
      "8600 950\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data in from the csv...\")\n",
    "personalities = pd.read_csv(INPUT_FILE)\n",
    "uniq_data = []\n",
    "uniq_handle = {}\n",
    "for index, row in personalities.iterrows():\n",
    "    if row[0] in uniq_handle:\n",
    "        continue\n",
    "    else:\n",
    "        uniq_handle[row[0]] = 1\n",
    "        uniq_data.append(row)\n",
    "\n",
    "print(\"Done reading data from csv\")\n",
    "        \n",
    "personalities = pd.DataFrame(uniq_data)\n",
    "#personalities.head()\n",
    "\n",
    "\n",
    "# Now, let's look at the different types of professions that we have and the number of samples that we have for each.\n",
    "\n",
    "# In[607]:\n",
    "\n",
    "professions = personalities.groupby('Profession')\n",
    "professions.size()\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "#    print(professions.size().order(ascending=False))\n",
    "\n",
    "\n",
    "# In[608]:\n",
    "\n",
    "data = np.array(personalities)\n",
    "\n",
    "\n",
    "# In[609]:\n",
    "\n",
    "anonymised_data = data[:,1:]\n",
    "\n",
    "\n",
    "# In[610]:\n",
    "\n",
    "data = defaultdict(list)\n",
    "\n",
    "\n",
    "# In[611]:\n",
    "\n",
    "for row in anonymised_data:\n",
    "    data[row[0]].append([float(r) for r in row[1:]])\n",
    "\n",
    "\n",
    "# In[612]:\n",
    "\n",
    "TOP = 10\n",
    "print(\"Number of professions to run models on:\", TOP)\n",
    "\n",
    "# In[613]:\n",
    "\n",
    "NUM_FOLDS=10\n",
    "TRAIN_CAP = 955\n",
    "HYPER_PARAMETER_TUNING_ITER = 1\n",
    "print(\"Running with\", NUM_FOLDS,\"folds\")\n",
    "\n",
    "\n",
    "# In[614]:\n",
    "\n",
    "counts = {}\n",
    "\n",
    "for key in data:\n",
    "    counts[key] = len(data[key])\n",
    "\n",
    "sorted_keys = [key for key in sorted(counts, \n",
    "                      key=counts.get, \n",
    "                      reverse=True)]\n",
    "\n",
    "\n",
    "sorted_keys = sorted_keys[:TOP]\n",
    "print(\"Professions we are using now: \")\n",
    "for prof in sorted_keys:\n",
    "    print(prof)\n",
    "# Random uniform variable -> Sample from uniform distribution of paramaters min 0 and max 1\n",
    "#                            Sample u from that\n",
    "#                            If sample is lower than 950/sizeofclass then keep sample otherwise throw sample away\n",
    "# sorted_keys = ['Athletics Director', 'Executive Chef', 'Data Scientist', 'Software Engineer', 'Event Coordinator', 'Health Manager', 'Literary Agent', 'Research Associate', 'Club Manager', 'Teacher']\n",
    "\n",
    "\n",
    "# In[617]:\n",
    "\n",
    "print(sorted_keys)\n",
    "\n",
    "# In[619]:\n",
    "\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=20)\n",
    "TRAIN_DATA=0\n",
    "TRAIN_LABELS=1\n",
    "TEST_DATA=2\n",
    "TEST_LABELS=3\n",
    "\n",
    "TRUE_LABEL = 0\n",
    "PREDICTED_LABEL = 1\n",
    "\n",
    "# In[620]:\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "\n",
    "# In[621]:\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for profession in sorted_keys:\n",
    "    choices = np.random.choice(len(data[profession]), TRAIN_CAP, replace=False)\n",
    "    \n",
    "    rand_data = [data[profession][i] for i in choices]\n",
    "    all_features += rand_data\n",
    "    all_labels += [profession for _ in range(len(rand_data))]\n",
    "\n",
    "choices = list(range(len(all_features)))\n",
    "shuffle(choices)\n",
    "\n",
    "all_features = [all_features[i] for i in choices]\n",
    "all_labels = [all_labels[i] for i in choices]\n",
    "index = 0\n",
    "print(\"Total number of samples:\", len(all_features))\n",
    "\n",
    "print(\"Splitting data into training and testing sets\")\n",
    "datasets = [[[],[],[],[]] for _ in range(NUM_FOLDS)]\n",
    "for train, test in skf.split(all_features, all_labels):\n",
    "    print(len(train), len(test))\n",
    "    datasets[index][TRAIN_DATA] = [all_features[i] for i in train]\n",
    "    datasets[index][TRAIN_LABELS] = [all_labels[i] for i in train]\n",
    "    datasets[index][TEST_DATA] = [all_features[i] for i in test]\n",
    "    datasets[index][TEST_LABELS] = [all_labels[i] for i in test]\n",
    "    \n",
    "    for li1 in datasets[index][TEST_DATA]:\n",
    "        for li2 in datasets[index][TRAIN_DATA]:\n",
    "            if li1 == li2:\n",
    "                print(\"ERROR: (DUPLICATE FOUND)\", li)\n",
    "                  \n",
    "    index += 1\n",
    "    \n",
    "no_features = len(pd.DataFrame(all_features).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {}\n",
    "# Only for randomised hyper parameter tuning\n",
    "tuned_parameters['KNN'] = {'n_neighbors' : list(range(10,200,5)), \n",
    "                           'leaf_size' : list(range(2, 200, 5)),\n",
    "                           'p' : [1,2]\n",
    "                          } \n",
    "tuned_parameters['random_forest'] = {'n_estimators' : list(range(10, 200, 5)),\n",
    "                                     'min_samples_leaf' : list(range(10, 200, 3)), \n",
    "                                     'criterion' : ['gini', 'entropy'], \n",
    "                                     'min_samples_split' : list(range(2, 200, 5))\n",
    "                                    }\n",
    "tuned_parameters['gradient_boosting'] = {'learning_rate' : [0.0001, 0.001, 0.1], \n",
    "                                         'n_estimators' : list(range(20, 200, 2)), \n",
    "                                         'max_features' : list(range(2, no_features))\n",
    "                                        }\n",
    "tuned_parameters['xgboost'] = {'n_estimators' : list(range(2, 300, 2)), \n",
    "                               'max_depth' : list(range(2, 100, 2)), \n",
    "                               'learning_rate' : [0.0001, 0.001, 0.01, 0.1],\n",
    "                               'subsample':[0.6,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "                              }\n",
    "\n",
    "# clfs = defaultdict(list)\n",
    "clfs = {}\n",
    "confusion_matrices = defaultdict(list)\n",
    "clfs['random_forest'] = RandomizedSearchCV(RandomForestClassifier(), \n",
    "                                           tuned_parameters['random_forest'], \n",
    "                                           n_jobs = -1, \n",
    "                                           n_iter=500,\n",
    "                                           cv = 5,\n",
    "                                           verbose = 1)\n",
    "clfs['gradient_boosting'] = RandomizedSearchCV(GradientBoostingClassifier(), \n",
    "                                              tuned_parameters['gradient_boosting'], \n",
    "                                              n_jobs = -1,\n",
    "                                              n_iter=500,\n",
    "                                              cv = 5,\n",
    "                                              verbose = 1)\n",
    "clfs['xgboost'] = RandomizedSearchCV(estimator = XGBClassifier(), \n",
    "                                     param_distributions = tuned_parameters['xgboost'], \n",
    "                                     n_jobs = -1,\n",
    "                                     n_iter=500,\n",
    "                                     cv = 5,\n",
    "                                     verbose = 1)\n",
    "clfs['logistic_regression'] = LogisticRegressionCV(Cs = [0.0001, 0.001, 0.1, 1.0, 10.0, 100, 1000],\n",
    "                                                   fit_intercept=True, \n",
    "                                                   n_jobs = -1,\n",
    "                                                   max_iter = 500,\n",
    "                                                   cv = 5,\n",
    "                                                   verbose = 1)\n",
    "clfs['KNN'] = RandomizedSearchCV(KNeighborsClassifier(), \n",
    "                                 tuned_parameters['KNN'], \n",
    "                                 n_jobs = -1, \n",
    "                                 n_iter=500,\n",
    "                                 cv = 5,\n",
    "                                 verbose = 1)\n",
    "\n",
    "true_labels = {}\n",
    "predicted_labels = {}\n",
    "scores = {}\n",
    "for classifier in clfs:\n",
    "    scores[classifier] = [[] for _ in range(NUM_FOLDS)]\n",
    "    true_labels[classifier] = [[] for _ in range(NUM_FOLDS)]\n",
    "    predicted_labels[classifier] = [[] for _ in range(NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting for all models: \n",
      "Training lr and in iteration:  0\n",
      "Training KNN and in iteration:  0\n",
      "Training lr and in iteration:  1\n",
      "Training KNN and in iteration:  1\n",
      "Training lr and in iteration:  2\n",
      "Training KNN and in iteration:  2\n",
      "Training lr and in iteration:  3\n",
      "Training KNN and in iteration:  3\n",
      "Training lr and in iteration:  4\n",
      "Training KNN and in iteration:  4\n",
      "Training lr and in iteration:  5\n",
      "Training KNN and in iteration:  5\n",
      "Training lr and in iteration:  6\n",
      "Training KNN and in iteration:  6\n",
      "Training lr and in iteration:  7\n",
      "Training KNN and in iteration:  7\n",
      "Training lr and in iteration:  8\n",
      "Training KNN and in iteration:  8\n",
      "Training lr and in iteration:  9\n",
      "Training KNN and in iteration:  9\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and predicting for all models: \")\n",
    "trained_clfs = [[] for _ in range(NUM_FOLDS)]\n",
    "for i, dataset in enumerate(datasets):\n",
    "    training_data = dataset[TRAIN_DATA]\n",
    "    training_labels = dataset[TRAIN_LABELS]\n",
    "    testing_data = dataset[TEST_DATA]\n",
    "    testing_labels = dataset[TEST_LABELS]\n",
    "    \n",
    "    ## initialise classifiers from list\n",
    "    trained_clfs[i] = copy.deepcopy(clfs)\n",
    "\n",
    "    for classifier in trained_clfs[i]:\n",
    "        print(\"--> Training\", classifier,\", fold: \",i)\n",
    "        trained_clfs[i][classifier].fit(np.array(training_data), training_labels)\n",
    "        predicted_labels[classifier][i] = trained_clfs[i][classifier].predict(np.array(testing_data))\n",
    "        true_labels[classifier][i] = testing_labels\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(testing_labels, predicted_labels[classifier][i], average='macro')\n",
    "        scores[classifier][i] = [prec, rec, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr [0.46676021 0.48598355 0.45449771]\n",
      "KNN [0.48219529 0.4890011  0.47432454]\n"
     ]
    }
   ],
   "source": [
    "all_scores = {}\n",
    "names = []\n",
    "for classifier in scores:\n",
    "    score = np.array([0,0,0])\n",
    "    for i in range(len(scores[classifier])):\n",
    "        score = score + np.array(scores[classifier][i])/NUM_FOLDS\n",
    "    all_scores[classifier] = score  \n",
    "\n",
    "for classifier in all_scores:\n",
    "    print(classifier, all_scores[classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully completed everything!\n"
     ]
    }
   ],
   "source": [
    "dill.dump_session(OUTPUT_FILE)\n",
    "print(\"Successfully completed everything!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
